{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd,time\n",
    "from selenium.webdriver import Safari\n",
    "from selenium.webdriver import DesiredCapabilities\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Function\n",
    "def naukri_da(url):\n",
    "    \n",
    "    #Create Blank Dictionary\n",
    "    job_dict={}\n",
    "    \n",
    "    #Create Safari Instance\n",
    "    browser=Safari()\n",
    "    \n",
    "    #Get URL\n",
    "    browser.get(url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    #Switch to current window\n",
    "    browser.switch_to.window(browser.current_window_handle)\n",
    "    \n",
    "    #Enter Data Analyst  and Banglore in Search jobs and location respectively and click on search\n",
    "    input_search_des=browser.find_element_by_xpath(\"//input[@class='sugInp' and @id='qsb-keyword-sugg']\")\n",
    "    input_search_des.send_keys('Data Analyst')\n",
    "    input_search_loc=browser.find_element_by_xpath(\"//input[@class='sugInp' and @id='qsb-location-sugg']\")\n",
    "    input_search_loc.send_keys('Bangalore')\n",
    "    click_search=browser.find_element_by_xpath(\"//div[@class='search-btn']\")\n",
    "    click_search.click()\n",
    "    time.sleep(2)    \n",
    "    \n",
    "    #Fetch response and create soup\n",
    "    response=browser.page_source\n",
    "    soup=BeautifulSoup(response,'lxml')\n",
    "    job_title=soup.find_all('div',attrs={'class':'list'})[0].find_all('div',attrs={'class':'info fleft'})\n",
    "    job_loc=soup.find_all('div',attrs={'class':'list'})[0].find_all('li',attrs={'class':'fleft grey-text br2 placeHolderLi location'})\n",
    "    job_ex=soup.find_all('div',attrs={'class':'list'})[0].find_all('li',attrs={'class':'fleft grey-text br2 placeHolderLi experience'})\n",
    "    job_comp=soup.find_all('div',attrs={'class':'mt-7 companyInfo subheading lh16'})\n",
    "    \n",
    "    #Update blank dictionary\n",
    "    job_dict.update({'Title':[]})\n",
    "    job_dict.update({'Location':[]})\n",
    "    job_dict.update({'Experience':[]})\n",
    "    job_dict.update({'Company':[]})\n",
    "    \n",
    "    #Append values in dicitionary\n",
    "    for x in range(0,10):\n",
    "        job_dict['Title'].append(job_title[x].a.text)\n",
    "        job_dict['Location'].append(job_loc[x].span.text)\n",
    "        job_dict['Experience'].append(job_ex[x].span.text)\n",
    "        job_dict['Company'].append(job_comp[x].a.text)\n",
    "    \n",
    "    #Create DataFrame and return the same\n",
    "    job_df=pd.DataFrame(job_dict)\n",
    "    return job_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fresher  Data Engineer / Data Scientist / Data...</td>\n",
       "      <td>Delhi NCR, Bengaluru, Hyderabad</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "      <td>ACHYUTAS SOFT PRIVATE LIMITED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Immediate opening For Data Scientist/Data Analyst</td>\n",
       "      <td>Chennai, Pune, Bengaluru, Hyderabad</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "      <td>CAIA-Center For Artificial Intelligence &amp; Adva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SAS AML Data Analyst / Trainee &amp; Data Science,...</td>\n",
       "      <td>Bengaluru(Whitefield)</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "      <td>MagicBase Royal BD Pvt Ltd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hiring Data Analysts</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>Flipkart Internet Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bengaluru, India</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "      <td>GlaxoSmithKline Pharmaceuticals Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Study Data Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "      <td>GlaxoSmithKline Pharmaceuticals Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>3-4 Yrs</td>\n",
       "      <td>Cognizant Technology Solutions India Ltd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>2-3 Yrs</td>\n",
       "      <td>Cognizant Technology Solutions India Ltd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Business Data Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "      <td>GENPACT India Private Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analyst ( Predictive Modelling)</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>AVE-Promagne</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Fresher  Data Engineer / Data Scientist / Data...   \n",
       "1  Immediate opening For Data Scientist/Data Analyst   \n",
       "2  SAS AML Data Analyst / Trainee & Data Science,...   \n",
       "3                               Hiring Data Analysts   \n",
       "4                                       Data Analyst   \n",
       "5                                 Study Data Analyst   \n",
       "6                                       Data Analyst   \n",
       "7                                       Data Analyst   \n",
       "8                              Business Data Analyst   \n",
       "9               Data Analyst ( Predictive Modelling)   \n",
       "\n",
       "                              Location Experience  \\\n",
       "0      Delhi NCR, Bengaluru, Hyderabad    0-2 Yrs   \n",
       "1  Chennai, Pune, Bengaluru, Hyderabad    0-3 Yrs   \n",
       "2                Bengaluru(Whitefield)    0-5 Yrs   \n",
       "3                            Bengaluru    2-5 Yrs   \n",
       "4                     Bengaluru, India    3-5 Yrs   \n",
       "5                            Bengaluru    4-8 Yrs   \n",
       "6                            Bengaluru    3-4 Yrs   \n",
       "7                            Bengaluru    2-3 Yrs   \n",
       "8                            Bengaluru    3-6 Yrs   \n",
       "9                            Bengaluru    2-5 Yrs   \n",
       "\n",
       "                                             Company  \n",
       "0                     ACHYUTAS SOFT PRIVATE LIMITED   \n",
       "1  CAIA-Center For Artificial Intelligence & Adva...  \n",
       "2                         MagicBase Royal BD Pvt Ltd  \n",
       "3                  Flipkart Internet Private Limited  \n",
       "4            GlaxoSmithKline Pharmaceuticals Limited  \n",
       "5            GlaxoSmithKline Pharmaceuticals Limited  \n",
       "6           Cognizant Technology Solutions India Ltd  \n",
       "7           Cognizant Technology Solutions India Ltd  \n",
       "8                      GENPACT India Private Limited  \n",
       "9                                      AVE-Promagne   "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Call function\n",
    "df=naukri_da(\"https://www.naukri.com\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, full job-description. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Function\n",
    "def naukri_ds(url):\n",
    "    \n",
    "    #Create Blank Dictionary\n",
    "    job_dict={}\n",
    "    \n",
    "    #Create Safari Instance\n",
    "    browser=Safari()\n",
    "    \n",
    "    #Get URL\n",
    "    browser.get(url)\n",
    "    time.sleep(2)\n",
    "    main_page=browser.current_window_handle\n",
    "    \n",
    "    #Close popups\n",
    "    for x in browser.window_handles:\n",
    "        if x!=main_page:\n",
    "            browser.switch_to.window(x)\n",
    "            browser.close()\n",
    "            time.sleep(1)\n",
    "    \n",
    "    #Switch to current window\n",
    "    browser.switch_to.window(main_page)\n",
    "    \n",
    "    #Enter Data Analyst  and Banglore in Search jobs and location respectively and click on search\n",
    "    input_search_des=browser.find_element_by_xpath(\"//input[@class='sugInp' and @id='qsb-keyword-sugg']\")\n",
    "    input_search_des.send_keys('Data Scientist')\n",
    "    input_search_loc=browser.find_element_by_xpath(\"//input[@class='sugInp' and @id='qsb-location-sugg']\")\n",
    "    input_search_loc.send_keys('Bangalore')\n",
    "    click_search=browser.find_element_by_xpath(\"//div[@class='search-btn']\")\n",
    "    click_search.click()\n",
    "    time.sleep(1)    \n",
    "    \n",
    "    #Fetch response and create soup\n",
    "    response=browser.page_source\n",
    "    soup=BeautifulSoup(response,'lxml')\n",
    "    job_title=soup.find_all('div',attrs={'class':'list'})[0].find_all('div',attrs={'class':'info fleft'})\n",
    "    job_loc=soup.find_all('div',attrs={'class':'list'})[0].find_all('li',attrs={'class':'fleft grey-text br2 placeHolderLi location'})\n",
    "    job_comp=soup.find_all('div',attrs={'class':'mt-7 companyInfo subheading lh16'})\n",
    "    \n",
    "    #Update blank dictionary\n",
    "    job_dict.update({'Title':[]})\n",
    "    job_dict.update({'Location':[]})\n",
    "    job_dict.update({'Company':[]})\n",
    "    job_dict.update({'Job Description':[]})\n",
    "    \n",
    "    #Append values in dicitionary\n",
    "    for x in range(0,10):\n",
    "        job_dict['Title'].append(job_title[x].a.text)\n",
    "        job_dict['Location'].append(job_loc[x].span.text)\n",
    "        job_dict['Company'].append(job_comp[x].a.text)\n",
    "    \n",
    "    time.sleep(1)\n",
    "    butn=browser.find_elements_by_xpath(\"//div[@class='info fleft']//a[@class='title fw500 ellipsis']\")\n",
    "    \n",
    "    second_page=browser.current_window_handle\n",
    "    \n",
    "    #Open each job and fetch Job Description\n",
    "    for x in range(0,10):\n",
    "        butn[x].click()\n",
    "        time.sleep(1)\n",
    "        for x in browser.window_handles:\n",
    "            if x != second_page:\n",
    "                browser.switch_to.window(x)\n",
    "                break\n",
    "        response=browser.page_source\n",
    "        soup=BeautifulSoup(response,'lxml')\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            p_test=soup.find_all('section',attrs={'class':'job-desc'})[0].find_all('p')\n",
    "            if len(p_test)==0:\n",
    "                y='Not able to fetch JD due to exception'\n",
    "            else:\n",
    "                y=''\n",
    "                for x in p_test:\n",
    "                    y=y+x.text.replace('<br/>',' ').replace('</strong>',' ')\n",
    "        except:\n",
    "            y='Not able to fetch JD due to exception'\n",
    "        browser.close()\n",
    "        time.sleep(2)\n",
    "        browser.switch_to.window(second_page)\n",
    "        job_dict['Job Description'].append(y)\n",
    "    \n",
    "    #Create DataFrame and return the same\n",
    "    job_df=pd.DataFrame(job_dict)\n",
    "    return job_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company</th>\n",
       "      <th>Job Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fresher  Data Engineer / Data Scientist / Data...</td>\n",
       "      <td>Delhi NCR, Bengaluru, Hyderabad</td>\n",
       "      <td>ACHYUTAS SOFT PRIVATE LIMITED</td>\n",
       "      <td>Roles and ResponsibilitiesAnalytical Skills:To...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Immediate opening For Data Scientist/Data Analyst</td>\n",
       "      <td>Chennai, Pune, Bengaluru, Hyderabad</td>\n",
       "      <td>CAIA-Center For Artificial Intelligence &amp; Adva...</td>\n",
       "      <td>Dear Candidate  Schedule a Telephonic Intervi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lead Data Scientist - Machine Learning/ Data M...</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Wrackle Technologies Pvt Ltd</td>\n",
       "      <td>Requirements :- 6-9 years of strong experience...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist - Machine Learning</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>BLUE YONDER INDIA PRIVATE LIMITED</td>\n",
       "      <td>Not able to fetch JD due to exception</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist / Data Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Altimetrik India Pvt. Ltd</td>\n",
       "      <td>Not able to fetch JD due to exception</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Senior Data Scientist - NLP/ Python/ R</td>\n",
       "      <td>Bengaluru, Hyderabad</td>\n",
       "      <td>AVI Consulting LLP</td>\n",
       "      <td>Skill : NLP,Semantic model, NER model, Deep Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Explore job openings on Data Scientist!!</td>\n",
       "      <td>Pune, Mumbai, Bengaluru</td>\n",
       "      <td>Bristlecone India Limited</td>\n",
       "      <td>JOB DESCRIPTION  Exp  4+ years 1) Hands-on py...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Signify</td>\n",
       "      <td>We re on the lookout for forward-thinking inn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Artificial Intelligence Analyst/Data Scientist</td>\n",
       "      <td>Mumbai, Bengaluru</td>\n",
       "      <td>TalentCo Search Pvt Ltd</td>\n",
       "      <td>- Machine Learning techniques (recommendation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Chennai, Pune, Mumbai, Bengaluru, Hyderabad, K...</td>\n",
       "      <td>Mailkit Private Limited</td>\n",
       "      <td>Mailkit is an European Marketing Automation co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Fresher  Data Engineer / Data Scientist / Data...   \n",
       "1  Immediate opening For Data Scientist/Data Analyst   \n",
       "2  Lead Data Scientist - Machine Learning/ Data M...   \n",
       "3                  Data Scientist - Machine Learning   \n",
       "4                      Data Scientist / Data Analyst   \n",
       "5             Senior Data Scientist - NLP/ Python/ R   \n",
       "6           Explore job openings on Data Scientist!!   \n",
       "7                              Senior Data Scientist   \n",
       "8     Artificial Intelligence Analyst/Data Scientist   \n",
       "9                                     Data Scientist   \n",
       "\n",
       "                                            Location  \\\n",
       "0                    Delhi NCR, Bengaluru, Hyderabad   \n",
       "1                Chennai, Pune, Bengaluru, Hyderabad   \n",
       "2                                          Bengaluru   \n",
       "3                                          Bengaluru   \n",
       "4                                          Bengaluru   \n",
       "5                               Bengaluru, Hyderabad   \n",
       "6                            Pune, Mumbai, Bengaluru   \n",
       "7                                          Bengaluru   \n",
       "8                                  Mumbai, Bengaluru   \n",
       "9  Chennai, Pune, Mumbai, Bengaluru, Hyderabad, K...   \n",
       "\n",
       "                                             Company  \\\n",
       "0                     ACHYUTAS SOFT PRIVATE LIMITED    \n",
       "1  CAIA-Center For Artificial Intelligence & Adva...   \n",
       "2                       Wrackle Technologies Pvt Ltd   \n",
       "3                  BLUE YONDER INDIA PRIVATE LIMITED   \n",
       "4                          Altimetrik India Pvt. Ltd   \n",
       "5                                 AVI Consulting LLP   \n",
       "6                          Bristlecone India Limited   \n",
       "7                                            Signify   \n",
       "8                            TalentCo Search Pvt Ltd   \n",
       "9                            Mailkit Private Limited   \n",
       "\n",
       "                                     Job Description  \n",
       "0  Roles and ResponsibilitiesAnalytical Skills:To...  \n",
       "1   Dear Candidate  Schedule a Telephonic Intervi...  \n",
       "2  Requirements :- 6-9 years of strong experience...  \n",
       "3              Not able to fetch JD due to exception  \n",
       "4              Not able to fetch JD due to exception  \n",
       "5  Skill : NLP,Semantic model, NER model, Deep Le...  \n",
       "6   JOB DESCRIPTION  Exp  4+ years 1) Hands-on py...  \n",
       "7   We re on the lookout for forward-thinking inn...  \n",
       "8  - Machine Learning techniques (recommendation ...  \n",
       "9  Mailkit is an European Marketing Automation co...  "
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=naukri_ds('https://www.naukri.com')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: In this question you have to scrape data using the filters available on the webpage as shown below: You have to use the location and salary filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser=Safari()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.naukri.com'\n",
    "browser.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.switch_to.window(browser.current_window_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_search_des=browser.find_element_by_xpath(\"//input[@class='sugInp' and @id='qsb-keyword-sugg']\")\n",
    "input_search_des.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_search=browser.find_element_by_xpath(\"//div[@class='search-btn']\")\n",
    "click_search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_delhi=browser.find_element_by_xpath(\"//label[@for='chk-Delhi/NCR-cityType-']//i\")\n",
    "click_delhi.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_sal=browser.find_element_by_xpath(\"//div[@data-filter-id='salaryRange']//label[@for='chk-3-6 Lakhs-ctcFilter-']//i\")\n",
    "ent_sal.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Function\n",
    "def naukri_da_filter(url):\n",
    "    \n",
    "    #Create Blank Dictionary\n",
    "    job_dict={}\n",
    "    \n",
    "    #Create Safari Instance\n",
    "    browser=Safari()\n",
    "    url='https://www.naukri.com'\n",
    "    \n",
    "    #Get URL\n",
    "    browser.get(url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    #Switch to current window\n",
    "    browser.switch_to.window(browser.current_window_handle)\n",
    "    \n",
    "    #Enter Data Analyst  and Banglore in Search jobs and location respectively and click on search\n",
    "    input_search_des=browser.find_element_by_xpath(\"//input[@class='sugInp' and @id='qsb-keyword-sugg']\")\n",
    "    input_search_des.send_keys('Data Scientist')\n",
    "    click_search=browser.find_element_by_xpath(\"//div[@class='search-btn']\")\n",
    "    click_search.click()\n",
    "    time.sleep(1)    \n",
    "    click_delhi=browser.find_element_by_xpath(\"//label[@for='chk-Delhi/NCR-cityType-']//i\")\n",
    "    click_delhi.click()\n",
    "    time.sleep(1)\n",
    "    ent_sal=browser.find_element_by_xpath(\"//div[@data-filter-id='salaryRange']//label[@for='chk-3-6 Lakhs-ctcFilter-']//i\")\n",
    "    ent_sal.click()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Fetch response and create soup\n",
    "    response=browser.page_source\n",
    "    soup=BeautifulSoup(response,'lxml')\n",
    "    job_title=soup.find_all('div',attrs={'class':'list'})[0].find_all('div',attrs={'class':'info fleft'})\n",
    "    job_loc=soup.find_all('div',attrs={'class':'list'})[0].find_all('li',attrs={'class':'fleft grey-text br2 placeHolderLi location'})\n",
    "    job_ex=soup.find_all('div',attrs={'class':'list'})[0].find_all('li',attrs={'class':'fleft grey-text br2 placeHolderLi experience'})\n",
    "    job_comp=soup.find_all('div',attrs={'class':'mt-7 companyInfo subheading lh16'})\n",
    "    \n",
    "    #Update blank dictionary\n",
    "    job_dict.update({'Title':[]})\n",
    "    job_dict.update({'Location':[]})\n",
    "    job_dict.update({'Experience':[]})\n",
    "    job_dict.update({'Company':[]})\n",
    "    \n",
    "    #Append values in dicitionary\n",
    "    for x in range(0,10):\n",
    "        job_dict['Title'].append(job_title[x].a.text)\n",
    "        job_dict['Location'].append(job_loc[x].span.text)\n",
    "        job_dict['Experience'].append(job_ex[x].span.text)\n",
    "        job_dict['Company'].append(job_comp[x].a.text)\n",
    "    \n",
    "    #Create DataFrame and return the same\n",
    "    job_df=pd.DataFrame(job_dict)\n",
    "    return job_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fresher  Data Engineer / Data Scientist / Data...</td>\n",
       "      <td>Delhi NCR, Bengaluru, Hyderabad</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "      <td>ACHYUTAS SOFT PRIVATE LIMITED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurgaon Gurugram</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "      <td>IBM India Pvt. Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist - Python/Machine Learning</td>\n",
       "      <td>Noida</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "      <td>Jubna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>5-9 Yrs</td>\n",
       "      <td>PureSoftware Pvt Ltd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Only Fresher / Data Scientist / Data Analyst /...</td>\n",
       "      <td>Delhi NCR, Greater Noida, Noida</td>\n",
       "      <td>0-0 Yrs</td>\n",
       "      <td>GABA Consultancy services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Delhi NCR, Gurgaon</td>\n",
       "      <td>1-4 Yrs</td>\n",
       "      <td>Air Asia India Limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>Delhi NCR, Noida(Sector-142 Noida)</td>\n",
       "      <td>9-14 Yrs</td>\n",
       "      <td>NEC CORPORATION INDIA PRIVATE LTD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Excellent opportunity For Lead Data Scientist ...</td>\n",
       "      <td>Delhi NCR(Sector-142 Noida), Noida</td>\n",
       "      <td>9-14 Yrs</td>\n",
       "      <td>NEC CORPORATION INDIA PRIVATE LTD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>Delhi NCR, Noida(Sector-142 Noida)</td>\n",
       "      <td>9-14 Yrs</td>\n",
       "      <td>NEC CORPORATION INDIA PRIVATE LTD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Excellent opportunity For Lead Data Scientist ...</td>\n",
       "      <td>Delhi NCR(Sector-142 Noida), Noida</td>\n",
       "      <td>9-14 Yrs</td>\n",
       "      <td>NEC CORPORATION INDIA PRIVATE LTD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Fresher  Data Engineer / Data Scientist / Data...   \n",
       "1                                     Data Scientist   \n",
       "2           Data Scientist - Python/Machine Learning   \n",
       "3                                     Data Scientist   \n",
       "4  Only Fresher / Data Scientist / Data Analyst /...   \n",
       "5                                     Data Scientist   \n",
       "6                                Lead Data Scientist   \n",
       "7  Excellent opportunity For Lead Data Scientist ...   \n",
       "8                                Lead Data Scientist   \n",
       "9  Excellent opportunity For Lead Data Scientist ...   \n",
       "\n",
       "                             Location Experience  \\\n",
       "0     Delhi NCR, Bengaluru, Hyderabad    0-2 Yrs   \n",
       "1                    Gurgaon Gurugram    4-8 Yrs   \n",
       "2                               Noida    5-8 Yrs   \n",
       "3                             Gurgaon    5-9 Yrs   \n",
       "4     Delhi NCR, Greater Noida, Noida    0-0 Yrs   \n",
       "5                  Delhi NCR, Gurgaon    1-4 Yrs   \n",
       "6  Delhi NCR, Noida(Sector-142 Noida)   9-14 Yrs   \n",
       "7  Delhi NCR(Sector-142 Noida), Noida   9-14 Yrs   \n",
       "8  Delhi NCR, Noida(Sector-142 Noida)   9-14 Yrs   \n",
       "9  Delhi NCR(Sector-142 Noida), Noida   9-14 Yrs   \n",
       "\n",
       "                             Company  \n",
       "0     ACHYUTAS SOFT PRIVATE LIMITED   \n",
       "1             IBM India Pvt. Limited  \n",
       "2                              Jubna  \n",
       "3              PureSoftware Pvt Ltd.  \n",
       "4         GABA Consultancy services   \n",
       "5             Air Asia India Limited  \n",
       "6  NEC CORPORATION INDIA PRIVATE LTD  \n",
       "7  NEC CORPORATION INDIA PRIVATE LTD  \n",
       "8  NEC CORPORATION INDIA PRIVATE LTD  \n",
       "9  NEC CORPORATION INDIA PRIVATE LTD  "
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=naukri_da_filter('https://www.naukri.com')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: Write a python program to scrape data for first 10 job results for Data scientist Designation in Noida location. You have to scrape company_name, No. of days ago when job was posted, Rating of the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glassdoor(url):\n",
    "     \n",
    "    #Create Blank Dictionary\n",
    "    job_dict={}\n",
    "    \n",
    "    #Create Safari Instance\n",
    "    browser=Safari()\n",
    "    \n",
    "    #Get URL\n",
    "    browser.get(url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    #Switch to current window\n",
    "    #browser.switch_to.window(browser.current_window_handle)\n",
    "    \n",
    "    #Enter Data Scientist and Noida in Search jobs and location respectively and click on search\n",
    "    job_search=browser.find_element_by_xpath(\"//input[@id='sc.keyword']\")\n",
    "    job_search.send_keys('Data Scientist')\n",
    "    loc_search=browser.find_element_by_xpath(\"//input[@id='sc.location']\")\n",
    "    loc_search.send_keys('Noida')\n",
    "    submit_btn=browser.find_element_by_xpath(\"//button[@type='submit']\")\n",
    "    submit_btn.click()\n",
    "    time.sleep(3)    \n",
    "    \n",
    "    #Fetch response and create soup\n",
    "    response=browser.page_source\n",
    "    soup=BeautifulSoup(response,'lxml')\n",
    "    comp_name=soup.find_all('article',attrs={'class':'noPad'})[0].find_all('div',attrs={'class':'d-flex flex-column pl-sm css-nq3w9f'})\n",
    "    days_posted=soup.find_all('div',attrs={'data-test':'job-age'})\n",
    "    ratings=soup.find_all('span',attrs={'class':'compactStars'})\n",
    "    \n",
    "    #Update blank dictionary\n",
    "    job_dict.update({'Company':[]})\n",
    "    job_dict.update({'Days Posted':[]})\n",
    "    job_dict.update({'Ratings':[]})\n",
    "    \n",
    "    #Append values in dicitionary\n",
    "    for x in range(0,10):\n",
    "        job_dict['Company'].append(comp_name[x].span.text)\n",
    "        job_dict['Days Posted'].append(days_posted[x].text)\n",
    "        job_dict['Ratings'].append(ratings[x].text)\n",
    "    \n",
    "    #Create DataFrame and return the same\n",
    "    job_df=pd.DataFrame(job_dict)\n",
    "    return job_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Days Posted</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NatWest Group</td>\n",
       "      <td>24h</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AI Engineer</td>\n",
       "      <td>8d</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ANI Calls India Private Limited</td>\n",
       "      <td>24h</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Microsoft</td>\n",
       "      <td>14d</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WSD Consultant</td>\n",
       "      <td>2d</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ANI Calls India Private Limited</td>\n",
       "      <td>24h</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ANI Calls India Private Limited</td>\n",
       "      <td>24h</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dürr AG</td>\n",
       "      <td>14d</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Applicate IT Solutions Pvt. Ltd.</td>\n",
       "      <td>3d</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dürr Somac GmbH</td>\n",
       "      <td>14d</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Company Days Posted Ratings\n",
       "0                     NatWest Group         24h     3.9\n",
       "1                       AI Engineer          8d     3.2\n",
       "2   ANI Calls India Private Limited         24h     4.4\n",
       "3                         Microsoft         14d     3.5\n",
       "4                    WSD Consultant          2d     3.3\n",
       "5   ANI Calls India Private Limited         24h     3.2\n",
       "6   ANI Calls India Private Limited         24h     4.4\n",
       "7                           Dürr AG         14d     3.3\n",
       "8  Applicate IT Solutions Pvt. Ltd.          3d     3.7\n",
       "9                   Dürr Somac GmbH         14d       4"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=glassdoor('https://www.glassdoor.co.in/Jobs/Glassdoor-Jobs-E100431.htm')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: Write a python program to scrape the salary data for Data Scientist designation in Noida location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glassdoor_avg(url):\n",
    "         \n",
    "    #Create Blank Dictionary\n",
    "    job_dict={}\n",
    "    \n",
    "    #Create Safari Instance\n",
    "    browser=Safari()\n",
    "    \n",
    "    #Get URL\n",
    "    browser.get(url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    #Switch to current window\n",
    "    #browser.switch_to.window(browser.current_window_handle)\n",
    "    \n",
    "    #Enter Data Scientist and Noida in Search jobs and location respectively and click on search\n",
    "    job_search=browser.find_element_by_xpath(\"//input[@name='sc.keyword']\")\n",
    "    job_search.send_keys('Data Scientist')\n",
    "    loc_search=browser.find_element_by_xpath(\"//input[@id='LocationSearch']\")\n",
    "    #Clear the defaults first\n",
    "    loc_search.clear()\n",
    "    loc_search.send_keys('Noida')\n",
    "    submit_btn=browser.find_element_by_xpath(\"//button[@id='HeroSearchButton']\") \n",
    "    submit_btn.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    \n",
    "    #Fetch response and create soup\n",
    "    response=browser.page_source\n",
    "    soup=BeautifulSoup(response,'lxml')\n",
    "    company=soup.find_all('div',attrs={'class':'module mb-0'})[0].find_all('div',attrs={'data-test':'job-info'})\n",
    "    avg_pay=soup.find_all('div',attrs={'class':'col-2 d-none d-md-flex flex-row justify-content-end'})\n",
    "    min_max=soup.find_all('div',attrs={'class':'col-3 offset-1 d-none d-md-block'})\n",
    "    \n",
    "    \n",
    "    #Update blank dictionary\n",
    "    job_dict.update({'Company':[]})\n",
    "    job_dict.update({'Average Pay':[]})\n",
    "    job_dict.update({'Minimum Sal':[]})\n",
    "    job_dict.update({'Maximum Sal':[]})\n",
    "    \n",
    "    for x in range(10):\n",
    "        job_dict['Company'].append(company[x].find_all('p',attrs={'class':'m-0'})[1].text)\n",
    "        job_dict['Average Pay'].append(avg_pay[x].strong.text)\n",
    "        job_dict['Minimum Sal'].append(min_max[x].find_all('div',attrs={'class':'common__RangeBarStyle__values common__flex__justifySpaceBetween common__flex__container'})[0].find_all('span')[0].text)\n",
    "        job_dict['Maximum Sal'].append(min_max[x].find_all('div',attrs={'class':'common__RangeBarStyle__values common__flex__justifySpaceBetween common__flex__container'})[0].find_all('span')[1].text)\n",
    "    \n",
    "    #Create DataFrame and return the same\n",
    "    job_df=pd.DataFrame(job_dict)\n",
    "    return job_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Average Pay</th>\n",
       "      <th>Minimum Sal</th>\n",
       "      <th>Maximum Sal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Delhivery</td>\n",
       "      <td>₹ 12,83,026</td>\n",
       "      <td>₹705K</td>\n",
       "      <td>₹11,495K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accenture</td>\n",
       "      <td>₹ 11,19,272</td>\n",
       "      <td>₹571K</td>\n",
       "      <td>₹2,200K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IBM</td>\n",
       "      <td>₹ 7,52,445</td>\n",
       "      <td>₹580K</td>\n",
       "      <td>₹2,700K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ericsson-Worldwide</td>\n",
       "      <td>₹ 8,28,000</td>\n",
       "      <td>₹468K</td>\n",
       "      <td>₹1,595K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UnitedHealth Group</td>\n",
       "      <td>₹ 13,21,601</td>\n",
       "      <td>₹708K</td>\n",
       "      <td>₹1,557K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Analytics Vidhya</td>\n",
       "      <td>₹ 20,889</td>\n",
       "      <td>₹14K</td>\n",
       "      <td>₹22K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tata Consultancy Services</td>\n",
       "      <td>₹ 6,33,432</td>\n",
       "      <td>₹488K</td>\n",
       "      <td>₹1,000K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Cognizant Technology Solutions</td>\n",
       "      <td>₹ 9,96,446</td>\n",
       "      <td>₹784K</td>\n",
       "      <td>₹1,250K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Valiance Solutions</td>\n",
       "      <td>₹ 7,71,320</td>\n",
       "      <td>₹496K</td>\n",
       "      <td>₹1,138K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vidooly Media Tech</td>\n",
       "      <td>₹ 12,669</td>\n",
       "      <td>₹8K</td>\n",
       "      <td>₹20K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Company  Average Pay Minimum Sal Maximum Sal\n",
       "0                       Delhivery  ₹ 12,83,026       ₹705K    ₹11,495K\n",
       "1                       Accenture  ₹ 11,19,272       ₹571K     ₹2,200K\n",
       "2                             IBM   ₹ 7,52,445       ₹580K     ₹2,700K\n",
       "3              Ericsson-Worldwide   ₹ 8,28,000       ₹468K     ₹1,595K\n",
       "4              UnitedHealth Group  ₹ 13,21,601       ₹708K     ₹1,557K\n",
       "5                Analytics Vidhya     ₹ 20,889        ₹14K        ₹22K\n",
       "6       Tata Consultancy Services   ₹ 6,33,432       ₹488K     ₹1,000K\n",
       "7  Cognizant Technology Solutions   ₹ 9,96,446       ₹784K     ₹1,250K\n",
       "8              Valiance Solutions   ₹ 7,71,320       ₹496K     ₹1,138K\n",
       "9              Vidooly Media Tech     ₹ 12,669         ₹8K        ₹20K"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=glassdoor_avg('https://www.glassdoor.co.in/Salaries/index.htm')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6 : Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flipkart_sunglass(url):\n",
    "             \n",
    "    #Create Blank Dictionary\n",
    "    flip={}\n",
    "    \n",
    "    #Create Safari Instance\n",
    "    browser=Safari()\n",
    "    \n",
    "    #Get URL\n",
    "    browser.get(url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    #Switch to current window\n",
    "    #browser.switch_to.window(browser.current_window_handle)\n",
    "    \n",
    "    #Enter Sunglasses in Search bar and click on search\n",
    "    click_x=browser.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\")\n",
    "    click_x.click()\n",
    "    input_text=browser.find_element_by_xpath(\"//input[@class='_3704LK']\")\n",
    "    input_text.send_keys('sunglasses')\n",
    "    click_search=browser.find_element_by_xpath(\"//button[@type='submit']\")\n",
    "    click_search.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    #Update blank dictionary\n",
    "    flip.update({'Brand':[]})\n",
    "    flip.update({'Price':[]})\n",
    "    flip.update({'Description':[]})\n",
    "    flip.update({'Discount':[]})\n",
    "    \n",
    "    response=browser.page_source\n",
    "    soup=BeautifulSoup(response,'lxml')\n",
    "    \n",
    "    brand=soup.find_all('div',attrs={'class':'_2WkVRV'})\n",
    "    desc=soup.find_all('div',attrs={'class':'_2B099V'})\n",
    "    price=soup.find_all('div',attrs={'class':'_25b18c'})\n",
    "    disc=soup.find_all('div',attrs={'class':'_25b18c'})\n",
    "    \n",
    "    for x in range(40):\n",
    "        flip['Brand'].append(brand[x].text)\n",
    "        flip['Description'].append(desc[x].a.text)\n",
    "        flip['Price'].append(price[x].find_all('div')[0].text)\n",
    "        flip['Discount'].append(disc[x].find_all('div')[2].text)\n",
    "    \n",
    "    def check_x(y):\n",
    "        if 100-len(y)<=0:\n",
    "            return None\n",
    "        elif 100-len(y)>40:\n",
    "            x=40\n",
    "            return x\n",
    "        else:\n",
    "            x=100-len(y)\n",
    "            return x\n",
    "    \n",
    "    x=check_x(flip['Brand'])\n",
    "    \n",
    "    click_next=browser.find_element_by_xpath(\"//div[@class='_2pi5LC col-12-12']//a[@class='_1LKTO3']\")\n",
    "    click_next.click()\n",
    "    response=browser.page_source\n",
    "    soup=BeautifulSoup(response,'lxml')\n",
    "    brand=soup.find_all('div',attrs={'class':'_2WkVRV'})\n",
    "    desc=soup.find_all('div',attrs={'class':'_2B099V'})\n",
    "    price=soup.find_all('div',attrs={'class':'_25b18c'})\n",
    "    disc=soup.find_all('div',attrs={'class':'_25b18c'})\n",
    "    \n",
    "    for x in range(x):\n",
    "        flip['Brand'].append(brand[x].text)\n",
    "        flip['Description'].append(desc[x].a.text)\n",
    "        flip['Price'].append(price[x].find_all('div')[0].text)\n",
    "        flip['Discount'].append(disc[x].find_all('div')[2].text)\n",
    "    \n",
    "    x=check_x(flip['Brand'])\n",
    "    \n",
    "    time.sleep(2)\n",
    "    click_next=browser.find_element_by_xpath(\"//div[@class='_2pi5LC col-12-12']//a[@class='_1LKTO3'][2]\")\n",
    "    click_next.click()\n",
    "    time.sleep(2)\n",
    "    response=browser.page_source\n",
    "    soup=BeautifulSoup(response,'lxml')\n",
    "    brand=soup.find_all('div',attrs={'class':'_2WkVRV'})\n",
    "    desc=soup.find_all('div',attrs={'class':'_2B099V'})\n",
    "    price=soup.find_all('div',attrs={'class':'_25b18c'})\n",
    "    disc=soup.find_all('div',attrs={'class':'_25b18c'})\n",
    "    \n",
    "    for x in range(x):\n",
    "        flip['Brand'].append(brand[x].text)\n",
    "        flip['Description'].append(desc[x].a.text)\n",
    "        flip['Price'].append(price[x].find_all('div')[0].text)\n",
    "        flip['Discount'].append(disc[x].find_all('div')[2].text)\n",
    "        \n",
    "    flip_df=pd.DataFrame(flip)\n",
    "    return flip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Price</th>\n",
       "      <th>Description</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spexra</td>\n",
       "      <td>₹206</td>\n",
       "      <td>UV Protection Spectacle  Sunglasses (Free Size)</td>\n",
       "      <td>84% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Singco India</td>\n",
       "      <td>₹179</td>\n",
       "      <td>UV Protection, Polarized Aviator Sunglasses (F...</td>\n",
       "      <td>74% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FDA COLLECTION</td>\n",
       "      <td>₹199</td>\n",
       "      <td>Gradient, Mirrored, UV Protection Round, Round...</td>\n",
       "      <td>84% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FDA COLLECTION</td>\n",
       "      <td>₹188</td>\n",
       "      <td>Gradient, Mirrored, UV Protection Round, Round...</td>\n",
       "      <td>87% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JOHAENA</td>\n",
       "      <td>₹160</td>\n",
       "      <td>Riding Glasses, UV Protection Oval Sunglasses ...</td>\n",
       "      <td>87% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>SRPM</td>\n",
       "      <td>₹274</td>\n",
       "      <td>UV Protection Round Sunglasses (53)</td>\n",
       "      <td>89% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Johaena</td>\n",
       "      <td>₹359</td>\n",
       "      <td>UV Protection Butterfly Sunglasses (Free Size)</td>\n",
       "      <td>85% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>I Flash</td>\n",
       "      <td>₹179</td>\n",
       "      <td>UV Protection, Polarized, Gradient Round Sungl...</td>\n",
       "      <td>85% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Funky Boys</td>\n",
       "      <td>₹224</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (53)</td>\n",
       "      <td>85% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Royal Son</td>\n",
       "      <td>₹711</td>\n",
       "      <td>Polarized Aviator Sunglasses (58)</td>\n",
       "      <td>64% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Brand Price                                        Description  \\\n",
       "0           Spexra  ₹206    UV Protection Spectacle  Sunglasses (Free Size)   \n",
       "1     Singco India  ₹179  UV Protection, Polarized Aviator Sunglasses (F...   \n",
       "2   FDA COLLECTION  ₹199  Gradient, Mirrored, UV Protection Round, Round...   \n",
       "3   FDA COLLECTION  ₹188  Gradient, Mirrored, UV Protection Round, Round...   \n",
       "4          JOHAENA  ₹160  Riding Glasses, UV Protection Oval Sunglasses ...   \n",
       "..             ...   ...                                                ...   \n",
       "95            SRPM  ₹274                UV Protection Round Sunglasses (53)   \n",
       "96         Johaena  ₹359     UV Protection Butterfly Sunglasses (Free Size)   \n",
       "97         I Flash  ₹179  UV Protection, Polarized, Gradient Round Sungl...   \n",
       "98      Funky Boys  ₹224             UV Protection Wayfarer Sunglasses (53)   \n",
       "99       Royal Son  ₹711                  Polarized Aviator Sunglasses (58)   \n",
       "\n",
       "   Discount  \n",
       "0   84% off  \n",
       "1   74% off  \n",
       "2   84% off  \n",
       "3   87% off  \n",
       "4   87% off  \n",
       "..      ...  \n",
       "95  89% off  \n",
       "96  85% off  \n",
       "97  85% off  \n",
       "98  85% off  \n",
       "99  64% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=flipkart_sunglass('https://www.flipkart.com')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_iphone(url):\n",
    "    iphone_dict={}\n",
    "    url_list=[]\n",
    "    \n",
    "    browser=Safari()\n",
    "    browser.get(url)\n",
    "    allrev=browser.find_element_by_xpath(\"//div[@class='_3UAT2v _16PBlm']\")\n",
    "    allrev.click()\n",
    "    time.sleep(2)\n",
    "    browser.refresh()\n",
    "    time.sleep(2)\n",
    "\n",
    "    \n",
    "    iphone_dict.update({'Summary':[]})\n",
    "    iphone_dict.update({'Description':[]})\n",
    "    iphone_dict.update({'Rating':[]})\n",
    "    \n",
    "    response=browser.page_source\n",
    "    soup=BeautifulSoup(response,'lxml')\n",
    "    \n",
    "    summary=soup.find_all('div',attrs={'class':'_1YokD2 _3Mn1Gg col-9-12'})[0].find_all('div',attrs={'class':'col _2wzgFH K0kLPL'})\n",
    "    desc=soup.find_all('div',attrs={'class':'_1YokD2 _3Mn1Gg col-9-12'})[0].find_all('div',attrs={'class':'col _2wzgFH K0kLPL'})\n",
    "    rating=soup.find_all('div',attrs={'class':'_1YokD2 _3Mn1Gg col-9-12'})[0].find_all('div',attrs={'class':'_3LWZlK _1BLPMq'})\n",
    "    \n",
    "    for x in range(10):\n",
    "        iphone_dict['Summary'].append(summary[x].p.text)\n",
    "        iphone_dict['Description'].append(desc[x].find('div',attrs={'class':''}).div.text)\n",
    "        iphone_dict['Rating'].append(rating[x].text)\n",
    "    \n",
    "    try:\n",
    "        click_next=browser.find_element_by_xpath(\"//div[@class='_2MImiq _1Qnn1K']//a[@class='_1LKTO3']\").get_attribute('href')\n",
    "        browser.get(click_next)\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        click_next=browser.find_element_by_xpath(\"//div[@class='_2MImiq _1Qnn1K']//a[@class='_1LKTO3']\").get_attribute('href')\n",
    "        browser.get(click_next)\n",
    "        time.sleep(2)\n",
    "    \n",
    "    while (len(iphone_dict['Summary'])<100):\n",
    "        for x in range(10):\n",
    "            iphone_dict['Summary'].append(summary[x].p.text) \n",
    "            iphone_dict['Description'].append(desc[x].find('div',attrs={'class':''}).div.text)\n",
    "            iphone_dict['Rating'].append(rating[x].text)\n",
    "        \n",
    "        try:\n",
    "            click_next=browser.find_element_by_xpath(\"//div[@class='_2MImiq _1Qnn1K']//a[@class='_1LKTO3'][2]\").get_attribute('href')\n",
    "            browser.get(click_next)\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            click_next=browser.find_element_by_xpath(\"//div[@class='_2MImiq _1Qnn1K']//a[@class='_1LKTO3'][2]\").get_attribute('href')\n",
    "            browser.get(click_next)\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "            \n",
    "        response=browser.page_source\n",
    "        soup=BeautifulSoup(response,'lxml')\n",
    "        summary=soup.find_all('div',attrs={'class':'_1YokD2 _3Mn1Gg col-9-12'})[0].find_all('div',attrs={'class':'col _2wzgFH K0kLPL'})\n",
    "        desc=soup.find_all('div',attrs={'class':'_1YokD2 _3Mn1Gg col-9-12'})[0].find_all('div',attrs={'class':'col _2wzgFH K0kLPL'})\n",
    "        rating=soup.find_all('div',attrs={'class':'_1YokD2 _3Mn1Gg col-9-12'})[0].find_all('div',attrs={'class':'_3LWZlK _1BLPMq'})\n",
    "    \n",
    "    iphone_df=pd.DataFrame(iphone_dict)\n",
    "    return iphone_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Description</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Great product</td>\n",
       "      <td>Amazing Powerful and Durable Gadget.I’m am ver...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>It’s a must buy who is looking for an upgrade ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>iphone 11 is a very good phone to buy only if ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Value for money❤️❤️Its awesome mobile phone in...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Nice</td>\n",
       "      <td>Iphone 11 black 64gb is really a cool phone 1....</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Nice</td>\n",
       "      <td>Although it’s an iPhone, it doesn’t give anyth...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Just wow!</td>\n",
       "      <td>Apple i Phone is the best phone available in t...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Brilliant</td>\n",
       "      <td>use outside gives a outstanding experience ......</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Delightful</td>\n",
       "      <td>Just the display held it from being a 5star ph...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Summary                                        Description  \\\n",
       "0     Perfect product!  Amazing phone with great cameras and better ba...   \n",
       "1        Great product  Amazing Powerful and Durable Gadget.I’m am ver...   \n",
       "2     Perfect product!  It’s a must buy who is looking for an upgrade ...   \n",
       "3   Highly recommended  iphone 11 is a very good phone to buy only if ...   \n",
       "4     Perfect product!  Value for money❤️❤️Its awesome mobile phone in...   \n",
       "..                 ...                                                ...   \n",
       "95                Nice  Iphone 11 black 64gb is really a cool phone 1....   \n",
       "96                Nice  Although it’s an iPhone, it doesn’t give anyth...   \n",
       "97           Just wow!  Apple i Phone is the best phone available in t...   \n",
       "98           Brilliant  use outside gives a outstanding experience ......   \n",
       "99          Delightful  Just the display held it from being a 5star ph...   \n",
       "\n",
       "   Rating  \n",
       "0       5  \n",
       "1       5  \n",
       "2       5  \n",
       "3       5  \n",
       "4       5  \n",
       "..    ...  \n",
       "95      3  \n",
       "96      3  \n",
       "97      5  \n",
       "98      5  \n",
       "99      4  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=flip_iphone('https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flipkart_sneakers(url):\n",
    "             \n",
    "    #Create Blank Dictionary\n",
    "    flip={}\n",
    "    \n",
    "    #Create Safari Instance\n",
    "    browser=Safari()\n",
    "    \n",
    "    #Get URL\n",
    "    browser.get(url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    #Enter Sneaker in Search bar and click on search\n",
    "    click_x=browser.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\")\n",
    "    click_x.click()\n",
    "    input_text=browser.find_element_by_xpath(\"//input[@class='_3704LK']\")\n",
    "    input_text.send_keys('Sneakers')\n",
    "    click_search=browser.find_element_by_xpath(\"//button[@type='submit']\")\n",
    "    click_search.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    #Update blank dictionary\n",
    "    flip.update({'Brand':[]})\n",
    "    flip.update({'Price':[]})\n",
    "    flip.update({'Description':[]})\n",
    "    flip.update({'Discount':[]})\n",
    "    \n",
    "    response=browser.page_source\n",
    "    soup=BeautifulSoup(response,'lxml')\n",
    "    \n",
    "    brand=soup.find_all('div',attrs={'class':'_2WkVRV'})\n",
    "    desc=soup.find_all('div',attrs={'class':'_2B099V'})\n",
    "    price=soup.find_all('div',attrs={'class':'_25b18c'})\n",
    "    disc=soup.find_all('div',attrs={'class':'_25b18c'})\n",
    "\n",
    "    \n",
    "    for x in range(0,40):\n",
    "        try:\n",
    "            flip['Brand'].append(brand[x].text)\n",
    "            flip['Description'].append(desc[x].a.text)\n",
    "            flip['Price'].append(price[x].find_all('div',attrs={'class':'_30jeq3'})[0].text)\n",
    "            flip['Discount'].append(disc[x].find_all('div',attrs={'class':'_3Ay6Sb'})[0].find('span').text)\n",
    "        except IndexError:\n",
    "            #In some cases there is no discount\n",
    "            flip['Discount'].append('-')\n",
    "            continue\n",
    "    \n",
    "    def check_x(y):\n",
    "        if 100-len(y)<=0:\n",
    "            return None\n",
    "        elif 100-len(y)>40:\n",
    "            x=40\n",
    "            return x\n",
    "        else:\n",
    "            x=100-len(y)\n",
    "            return x\n",
    "    \n",
    "    x=check_x(flip['Brand'])\n",
    "    \n",
    "    click_next=browser.find_element_by_xpath(\"//div[@class='_2pi5LC col-12-12']//a[@class='_1LKTO3']\")\n",
    "    click_next.click()\n",
    "    time.sleep(2)\n",
    "    response=browser.page_source\n",
    "    soup=BeautifulSoup(response,'lxml')\n",
    "    brand=soup.find_all('div',attrs={'class':'_2WkVRV'})\n",
    "    desc=soup.find_all('div',attrs={'class':'_2B099V'})\n",
    "    price=soup.find_all('div',attrs={'class':'_25b18c'})\n",
    "    disc=soup.find_all('div',attrs={'class':'_25b18c'})\n",
    "\n",
    "    \n",
    "    for x in range(x):\n",
    "        try:\n",
    "            flip['Brand'].append(brand[x].text)\n",
    "            flip['Description'].append(desc[x].a.text)\n",
    "            flip['Price'].append(price[x].find_all('div',attrs={'class':'_30jeq3'})[0].text)\n",
    "            flip['Discount'].append(disc[x].find_all('div',attrs={'class':'_3Ay6Sb'})[0].find('span').text)\n",
    "        except IndexError:\n",
    "            flip['Discount'].append('-')\n",
    "            continue\n",
    "\n",
    "    \n",
    "    x=check_x(flip['Brand'])\n",
    "    \n",
    "    time.sleep(2)\n",
    "    click_next=browser.find_element_by_xpath(\"//div[@class='_2pi5LC col-12-12']//a[@class='_1LKTO3'][2]\")\n",
    "    click_next.click()\n",
    "    time.sleep(2)\n",
    "    response=browser.page_source\n",
    "    soup=BeautifulSoup(response,'lxml')\n",
    "    brand=soup.find_all('div',attrs={'class':'_2WkVRV'})\n",
    "    desc=soup.find_all('div',attrs={'class':'_2B099V'})\n",
    "    price=soup.find_all('div',attrs={'class':'_25b18c'})\n",
    "    disc=soup.find_all('div',attrs={'class':'_25b18c'})\n",
    "    \n",
    "    for x in range(x):\n",
    "        try:\n",
    "            flip['Brand'].append(brand[x].text)\n",
    "            flip['Description'].append(desc[x].a.text)\n",
    "            flip['Price'].append(price[x].find_all('div',attrs={'class':'_30jeq3'})[0].text)\n",
    "            flip['Discount'].append(disc[x].find_all('div',attrs={'class':'_3Ay6Sb'})[0].find('span').text)\n",
    "        except IndexError:\n",
    "            flip['Discount'].append('-')\n",
    "            continue\n",
    "        \n",
    "        \n",
    "    flip_df=pd.DataFrame(flip)\n",
    "    return flip_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Price</th>\n",
       "      <th>Description</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rockfield</td>\n",
       "      <td>₹448</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sparx</td>\n",
       "      <td>₹730</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>₹236</td>\n",
       "      <td>171 Smart Tan Lace-Ups Casuals for Men Sneaker...</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>World Wear Footwear</td>\n",
       "      <td>₹240</td>\n",
       "      <td>5011-Latest Collection Stylish Casual Loafer S...</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Deny Brown</td>\n",
       "      <td>₹379</td>\n",
       "      <td>445 Sports Shoes Sneakers For Men</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>T-Rock</td>\n",
       "      <td>₹399</td>\n",
       "      <td>Mesh Casual Stylish Partywear Casaul Shoes For...</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Nike</td>\n",
       "      <td>₹2,112</td>\n",
       "      <td>Sb Charge Slr Sneakers For Men</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Sparx</td>\n",
       "      <td>₹649</td>\n",
       "      <td>Canvas Casual Partywear Outdoor Sneakers Shoes...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Puma</td>\n",
       "      <td>₹1,529</td>\n",
       "      <td>Court Breaker Derby Sneakers For Men</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Bonexy</td>\n",
       "      <td>₹299</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Brand   Price  \\\n",
       "0             Rockfield    ₹448   \n",
       "1                 Sparx    ₹730   \n",
       "2                Chevit    ₹236   \n",
       "3   World Wear Footwear    ₹240   \n",
       "4            Deny Brown    ₹379   \n",
       "..                  ...     ...   \n",
       "95               T-Rock    ₹399   \n",
       "96                 Nike  ₹2,112   \n",
       "97                Sparx    ₹649   \n",
       "98                 Puma  ₹1,529   \n",
       "99               Bonexy    ₹299   \n",
       "\n",
       "                                          Description Discount  \n",
       "0                                    Sneakers For Men       55  \n",
       "1                                    Sneakers For Men       23  \n",
       "2   171 Smart Tan Lace-Ups Casuals for Men Sneaker...       52  \n",
       "3   5011-Latest Collection Stylish Casual Loafer S...       51  \n",
       "4                   445 Sports Shoes Sneakers For Men       24  \n",
       "..                                                ...      ...  \n",
       "95  Mesh Casual Stylish Partywear Casaul Shoes For...       60  \n",
       "96                     Sb Charge Slr Sneakers For Men       53  \n",
       "97  Canvas Casual Partywear Outdoor Sneakers Shoes...        7  \n",
       "98               Court Breaker Derby Sneakers For Men       66  \n",
       "99                                   Sneakers For Men       70  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=flipkart_sneakers('https://www.flipkart.com')\n",
    "df['Discount']=df['Discount'].apply(lambda x:x[:x.find('%')])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9: Go to the link - https://www.myntra.com/shoes Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black”, as shown in the below image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myntra(url):\n",
    "    \n",
    "    #Create Dictionary\n",
    "    myntra_dict={}\n",
    "    \n",
    "    #Open Webpage\n",
    "    browser=Safari()\n",
    "    browser.get(url)\n",
    "    \n",
    "    #Click on filters\n",
    "    click_filt=browser.find_element_by_xpath(\"//input[@value='5899.0 TO 11599.0']/following-sibling::div\")\n",
    "    click_black=browser.find_element_by_xpath(\"//li[@class='colour-listItem']//input[@value='Black']/following-sibling::div\")\n",
    "    click_black.click()\n",
    "    time.sleep(5)    \n",
    "    click_filt.click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "    #Create Soup\n",
    "    response=browser.page_source\n",
    "    soup=BeautifulSoup(response,'lxml')\n",
    "    \n",
    "    #Create List in Dictionary\n",
    "    myntra_dict.update({'Brand':[]})\n",
    "    myntra_dict.update({'Description':[]})\n",
    "    myntra_dict.update({'Price':[]})\n",
    "    \n",
    "    #Create Variables\n",
    "    brand=soup.find_all('div',attrs={'class':'product-productMetaInfo'})\n",
    "    desc=soup.find_all('div',attrs={'class':'product-productMetaInfo'})\n",
    "    price=soup.find_all('div',attrs={'class':'product-productMetaInfo'})\n",
    "    \n",
    "    #Update List in for loop\n",
    "    for x in range(50):\n",
    "        myntra_dict['Brand'].append(brand[x].h3.text)\n",
    "        myntra_dict['Description'].append(desc[x].h4.text)\n",
    "        myntra_dict['Price'].append(price[x].find_all('div',attrs={'class':'product-price'})[0].span.text)\n",
    "    \n",
    "    \n",
    "    #Click on Next\n",
    "    click_next=browser.find_element_by_xpath(\"//li[@class='pagination-next']//a\").get_attribute('href')\n",
    "    browser.get(click_next)\n",
    "    time.sleep(1)\n",
    "    \n",
    "     #Create Soup\n",
    "    response=browser.page_source\n",
    "    soup=BeautifulSoup(response,'lxml')    \n",
    "    \n",
    "    #Create Variables\n",
    "    brand=soup.find_all('div',attrs={'class':'product-productMetaInfo'})\n",
    "    desc=soup.find_all('div',attrs={'class':'product-productMetaInfo'})\n",
    "    price=soup.find_all('div',attrs={'class':'product-productMetaInfo'})\n",
    "    \n",
    "    #Update List in for loop\n",
    "    for x in range(50):\n",
    "        myntra_dict['Brand'].append(brand[x].h3.text)\n",
    "        myntra_dict['Description'].append(desc[x].h4.text)\n",
    "        myntra_dict['Price'].append(price[x].find_all('div',attrs={'class':'product-price'})[0].span.text)\n",
    "        \n",
    "    myntra_df=pd.DataFrame(myntra_dict)\n",
    "    return myntra_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men QUEST 3 Running Shoes</td>\n",
       "      <td>5995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADIDAS Originals</td>\n",
       "      <td>Men Yung- 1 Sneakers</td>\n",
       "      <td>10999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KIPRUN By Decathlon</td>\n",
       "      <td>Men Kiprun Fast 2 Running</td>\n",
       "      <td>6399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Puma</td>\n",
       "      <td>Men HYBRID NETFIT Running Shoe</td>\n",
       "      <td>6599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men Zoom Running Shoes</td>\n",
       "      <td>7995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Puma</td>\n",
       "      <td>Men Hydra Running Shoes</td>\n",
       "      <td>7199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ADIDAS Originals</td>\n",
       "      <td>Men Suede Sneakers</td>\n",
       "      <td>6999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ADIDAS Originals</td>\n",
       "      <td>Men YUNG-1 Sneakers</td>\n",
       "      <td>10999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Cole Haan</td>\n",
       "      <td>Women Leather Sneakers</td>\n",
       "      <td>6599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>UNDER ARMOUR</td>\n",
       "      <td>Charged Pursuit 2 Running</td>\n",
       "      <td>6299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Brand                     Description  Price\n",
       "0                  Nike       Men QUEST 3 Running Shoes   5995\n",
       "1      ADIDAS Originals            Men Yung- 1 Sneakers  10999\n",
       "2   KIPRUN By Decathlon       Men Kiprun Fast 2 Running   6399\n",
       "3                  Puma  Men HYBRID NETFIT Running Shoe   6599\n",
       "4                  Nike          Men Zoom Running Shoes   7995\n",
       "..                  ...                             ...    ...\n",
       "95                 Puma         Men Hydra Running Shoes   7199\n",
       "96     ADIDAS Originals              Men Suede Sneakers   6999\n",
       "97     ADIDAS Originals             Men YUNG-1 Sneakers  10999\n",
       "98            Cole Haan          Women Leather Sneakers   6599\n",
       "99         UNDER ARMOUR       Charged Pursuit 2 Running   6299\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=myntra('https://www.myntra.com/shoes')\n",
    "df['Price']=df['Price'].apply(lambda x:x[x.find('Rs.')+4:9].replace('R','').strip())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon.Then set CPU Type filter to “Intel Core i7” and “Intel Core i9”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amazon(url):\n",
    "    \n",
    "    #Create Dictionary\n",
    "    amazon={}\n",
    "    \n",
    "    #Open Webpage\n",
    "    browser=Safari()\n",
    "    browser.get(url)\n",
    "    \n",
    "    #Enter Text\n",
    "    input_text=browser.find_element_by_xpath(\"//input[@class='nav-input' and @id='twotabsearchtextbox']\")\n",
    "    input_text.send_keys('Laptop')\n",
    "    click_search=browser.find_element_by_xpath(\"//input[@class='nav-input' and @type='submit']\")\n",
    "    click_search.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    #Click on i9. We will be clickin on i9 first then get results then clear the filter and then \n",
    "    #we will click on i7 and get the results as\n",
    "    #in site we can only click on one value of processor filter\n",
    "    clicki9=browser.find_element_by_xpath(\"//li[@aria-label='Intel Core i9']//i\")\n",
    "    clicki9.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    #Fetch response and create soup\n",
    "    response=browser.page_source\n",
    "    soup=BeautifulSoup(response,'lxml')\n",
    "    \n",
    "    #Initiate Dictionary\n",
    "    amazon.update({'Title':[]})\n",
    "    amazon.update({'Ratings':[]})\n",
    "    amazon.update({'Price':[]})\n",
    "    \n",
    "    #Get Variables\n",
    "    title=soup.find_all('div',attrs={'class':'sg-col-16-of-20 sg-col sg-col-8-of-12 sg-col-12-of-16'})[0].find_all('span',attrs={'class':'a-size-medium a-color-base a-text-normal'})\n",
    "    ratings=soup.find_all('div',attrs={'class':'sg-col-16-of-20 sg-col sg-col-8-of-12 sg-col-12-of-16'})[0].find_all('span',attrs={'class':'a-icon-alt'})\n",
    "    price=soup.find_all('div',attrs={'class':'sg-col-16-of-20 sg-col sg-col-8-of-12 sg-col-12-of-16'})[0].find_all('span',attrs={'class':'a-price-whole'})\n",
    "    \n",
    "    #Fetch Results\n",
    "    for x in range(5):\n",
    "        amazon['Title'].append(title[x].text)\n",
    "        amazon['Ratings'].append(ratings[x].text)\n",
    "        amazon['Price'].append(price[x].text)\n",
    "    \n",
    "    #Clear filter and select i7 and then repeat the process\n",
    "    clickclear=browser.find_element_by_xpath(\"//a[@class='a-link-normal s-navigation-item s-navigation-clear-link']//span[@class='a-size-base a-color-base']\")\n",
    "    clickclear.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    clicki7=browser.find_element_by_xpath(\"//li[@aria-label='Intel Core i7']//i\")\n",
    "    clicki7.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    #Fetch response and create soup\n",
    "    response=browser.page_source\n",
    "    soup=BeautifulSoup(response,'lxml')\n",
    "    \n",
    "    #Get Variables\n",
    "    title=soup.find_all('div',attrs={'class':'sg-col-16-of-20 sg-col sg-col-8-of-12 sg-col-12-of-16'})[0].find_all('span',attrs={'class':'a-size-medium a-color-base a-text-normal'})\n",
    "    ratings=soup.find_all('div',attrs={'class':'sg-col-16-of-20 sg-col sg-col-8-of-12 sg-col-12-of-16'})[0].find_all('span',attrs={'class':'a-icon-alt'})\n",
    "    price=soup.find_all('div',attrs={'class':'sg-col-16-of-20 sg-col sg-col-8-of-12 sg-col-12-of-16'})[0].find_all('span',attrs={'class':'a-price-whole'})\n",
    "    \n",
    "    #Fetch Results\n",
    "    for x in range(5):\n",
    "        amazon['Title'].append(title[x].text)\n",
    "        amazon['Ratings'].append(ratings[x].text)\n",
    "        amazon['Price'].append(price[x].text)\n",
    "    \n",
    "    amazon_df=pd.DataFrame(amazon)\n",
    "    return amazon_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASUS ZenBook Pro Duo Intel Core i9-10980HK 10t...</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>2,69,290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASUS ZenBook Pro Duo Intel Core i9-10980HK 10t...</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>2,69,290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dell Alienware M15 R2 15.6-inch FHD Laptop (9t...</td>\n",
       "      <td>4.7 out of 5 stars</td>\n",
       "      <td>2,44,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HP Omen X 2S Core i9 9th Gen 15.6-inch Dual Sc...</td>\n",
       "      <td>3.7 out of 5 stars</td>\n",
       "      <td>3,28,139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Renewed) Dell G7 15 Gaming 7588 Laptop Intel ...</td>\n",
       "      <td>2.8 out of 5 stars</td>\n",
       "      <td>1,07,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ASUS ZenBook 14 (2020) Intel Core i7-1165G7 11...</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>95,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HP Omen 10th Gen Intel Core i7 Processor 15.6-...</td>\n",
       "      <td>3.6 out of 5 stars</td>\n",
       "      <td>1,23,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(Renewed) Dell Inspiron 3567 Laptop Core i3-7t...</td>\n",
       "      <td>4.1 out of 5 stars</td>\n",
       "      <td>29,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(Renewed) Lenovo ThinkPad E14 Intel Core i7 10...</td>\n",
       "      <td>3.8 out of 5 stars</td>\n",
       "      <td>68,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HP Pavilion x360 Core i7 8th Gen 14-inch Touch...</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>85,990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title             Ratings  \\\n",
       "0  ASUS ZenBook Pro Duo Intel Core i9-10980HK 10t...  5.0 out of 5 stars   \n",
       "1  ASUS ZenBook Pro Duo Intel Core i9-10980HK 10t...  5.0 out of 5 stars   \n",
       "2  Dell Alienware M15 R2 15.6-inch FHD Laptop (9t...  4.7 out of 5 stars   \n",
       "3  HP Omen X 2S Core i9 9th Gen 15.6-inch Dual Sc...  3.7 out of 5 stars   \n",
       "4  (Renewed) Dell G7 15 Gaming 7588 Laptop Intel ...  2.8 out of 5 stars   \n",
       "5  ASUS ZenBook 14 (2020) Intel Core i7-1165G7 11...  5.0 out of 5 stars   \n",
       "6  HP Omen 10th Gen Intel Core i7 Processor 15.6-...  3.6 out of 5 stars   \n",
       "7  (Renewed) Dell Inspiron 3567 Laptop Core i3-7t...  4.1 out of 5 stars   \n",
       "8  (Renewed) Lenovo ThinkPad E14 Intel Core i7 10...  3.8 out of 5 stars   \n",
       "9  HP Pavilion x360 Core i7 8th Gen 14-inch Touch...  5.0 out of 5 stars   \n",
       "\n",
       "      Price  \n",
       "0  2,69,290  \n",
       "1  2,69,290  \n",
       "2  2,44,990  \n",
       "3  3,28,139  \n",
       "4  1,07,490  \n",
       "5    95,490  \n",
       "6  1,23,990  \n",
       "7    29,990  \n",
       "8    68,000  \n",
       "9    85,990  "
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=amazon('https://www.amazon.in/')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
